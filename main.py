from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import asyncio
import os
import json
import google.generativeai as genai
from neo4j_manager import Neo4jManager
from dotenv import load_dotenv
from contextlib import asynccontextmanager

load_dotenv()

# --- ì„¤ì • ë° ì´ˆê¸°í™” ---
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel('gemini-2.5-flash')

db = Neo4jManager()

# ì „ì—­ ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ
sim_state = {
    "traffic_level": 1.0,
    "events": []
}

# --- ë°±ê·¸ë¼ìš´ë“œ ì‹œë®¬ë ˆì´í„° ---
async def run_simulation_loop():
    print("ğŸš€ [System] Simulation Engine Started")
    while True:
        try:
            # 1. ì‹œë®¬ë ˆì´ì…˜ ë¬¼ë¦¬ ì—”ì§„ ê°€ë™ (ì´ë™, ë°°í„°ë¦¬ ì†Œëª¨)
            db.update_simulation_step(traffic_level=sim_state["traffic_level"])
            
            # 2. ëœë¤ ì´ë²¤íŠ¸ ìƒì„±ê¸° (ì‡¼í•‘ëª° ì—°ë™ ì‹œëŠ‰)
            import random
            if random.random() < 0.02: # 2% í™•ë¥ ë¡œ ì´ë²¤íŠ¸ ë°œìƒ
                if sim_state["traffic_level"] == 1.0:
                    evt_title = "âš¡ ê¹œì§ íƒ€ì„ì„¸ì¼ ì‹œì‘!"
                    evt_desc = "ì£¼ë¬¸ëŸ‰ 300% í­ì¦ ì˜ˆìƒ"
                    sim_state["traffic_level"] = 3.0
                    sim_state["events"].insert(0, {"title": evt_title, "desc": evt_desc, "type": "warning"})
                    db.inject_event("PROMOTION", "Traffic Surge")
                else:
                    evt_title = "âœ… ì„¸ì¼ ì¢…ë£Œ"
                    evt_desc = "ë¬¼ë™ëŸ‰ ì •ìƒí™”"
                    sim_state["traffic_level"] = 1.0
                    sim_state["events"].insert(0, {"title": evt_title, "desc": evt_desc, "type": "info"})
            
            # ì´ë²¤íŠ¸ ë¡œê·¸ëŠ” ìµœê·¼ 10ê°œë§Œ ìœ ì§€
            if len(sim_state["events"]) > 10:
                sim_state["events"] = sim_state["events"][:10]
                
            await asyncio.sleep(1.5) # 1.5ì´ˆë§ˆë‹¤ ê°±ì‹ 
        except Exception as e:
            print(f"âš ï¸ Sim Error: {e}")
            await asyncio.sleep(5)

@asynccontextmanager
async def lifespan(app: FastAPI):
    db.init_ontology() # ì„œë²„ ì‹œì‘ ì‹œ ì˜¨í†¨ë¡œì§€ ë¦¬ì…‹
    task = asyncio.create_task(run_simulation_loop())
    yield
    task.cancel()
    db.close()

app = FastAPI(lifespan=lifespan)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ í†µì‹  í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/api/dashboard")
def get_dashboard():
    """í”„ë¡ íŠ¸ì—”ë“œê°€ 1ì´ˆë§ˆë‹¤ í˜¸ì¶œ: ê·¸ë˜í”„ ë°ì´í„° + ì´ë²¤íŠ¸ ë¡œê·¸"""
    data = db.get_dashboard_data()
    return {
        "graph": data,
        "events": sim_state["events"],
        "traffic_level": sim_state["traffic_level"]
    }

class ChatRequest(BaseModel):
    message: str

@app.post("/api/chat")
async def chat_agent(req: ChatRequest):
    """
    RAG Agent: í˜„ì¬ ê·¸ë˜í”„ ìƒí™©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì£¼ì…ë°›ì•„ ë‹µë³€í•˜ê³ ,
    ì‹œê°í™”ë¥¼ ìœ„í•´ ê´€ë ¨ëœ ë…¸ë“œ IDë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•¨.
    """
    # 1. í˜„ì¬ ìƒí™© ìŠ¤ëƒ…ìƒ· ê°€ì ¸ì˜¤ê¸°
    context_data = db.get_context_for_llm()
    
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Toss ìŠ¤íƒ€ì¼ í˜ë¥´ì†Œë‚˜)
    system_prompt = f"""
    ë‹¹ì‹ ì€ ìµœì²¨ë‹¨ ë¬¼ë¥˜ì„¼í„° ê´€ì œ AIì…ë‹ˆë‹¤. 
    í˜„ì¬ ë¬¼ë¥˜ì„¼í„° ìƒí™© ë°ì´í„°: {json.dumps(context_data, ensure_ascii=False)}
    
    [ì§€ì‹œì‚¬í•­]
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìœ„ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    2. ë§íˆ¬ëŠ” 'í† ìŠ¤(Toss)' ì•±ì²˜ëŸ¼ ì •ì¤‘í•˜ê³ , ê°„ê²°í•˜ê³ , ëª…í™•í•˜ê²Œ í•˜ì„¸ìš”. (ì˜ˆ: "~ì…ë‹ˆë‹¤", "~í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤")
    3. ë‹µë³€ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì˜¨í†¨ë¡œì§€ ë…¸ë“œ IDê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì¶”ì¶œí•˜ì„¸ìš”.
    
    [ì¶œë ¥ í˜•ì‹]
    ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì“°ì§€ ë§ˆì„¸ìš”.
    {{
        "reply": "ì‚¬ìš©ìì—ê²Œ í•  ë‹µë³€ ë‚´ìš©",
        "related_nodes": ["ê´€ë ¨ëœ_ë…¸ë“œID_1", "ê´€ë ¨ëœ_ë…¸ë“œID_2"]
    }}
    """
    
    try:
        response = model.generate_content(f"{system_prompt}\nì‚¬ìš©ì ì§ˆë¬¸: {req.message}")
        # JSON íŒŒì‹± (Geminiê°€ ê°€ë” ë§ˆí¬ë‹¤ìš´ ```json ... ```ì„ ë¶™ì¼ ìˆ˜ ìˆìŒ)
        text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except Exception as e:
        print(f"LLM Error: {e}")
        return {
            "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì—°ê²° ìƒíƒœê°€ ë¶ˆì•ˆì •í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 
            "related_nodes": []
        }

# ì •ì  íŒŒì¼ ì„œë¹™ (ë°°í¬ ì‹œ React ë¹Œë“œ íŒŒì¼ ì—°ê²°)
app.mount("/", StaticFiles(directory="static", html=True), name="static")
